{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 : Import Libraries and Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and dependencies\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, Birch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Read csv file and create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas DataFrame\n",
    "df_market_data = pd.read_csv(\n",
    "    \"Resources/crypto_market_data.csv\",\n",
    "    index_col=\"coin_id\")\n",
    "\n",
    "# Display sample data\n",
    "df_market_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "df_market_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your data to see what's in your DataFrame\n",
    "df_market_data.hvplot.line(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    rot=90\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Data \n",
    "**Scale the DataFrame and create a new DataFrame that contains the scaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `StandardScaler()` module from scikit-learn to normalize the data from the CSV file\n",
    "market_data_scaled = StandardScaler().fit_transform(df_market_data[['price_change_percentage_24h', 'price_change_percentage_7d',\n",
    "       'price_change_percentage_14d', 'price_change_percentage_30d',\n",
    "       'price_change_percentage_60d', 'price_change_percentage_200d',\n",
    "       'price_change_percentage_1y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scaled data\n",
    "df_market_data_scaled = pd.DataFrame(\n",
    "    market_data_scaled, columns= ['price_change_percentage_24h', 'price_change_percentage_7d',\n",
    "       'price_change_percentage_14d', 'price_change_percentage_30d',\n",
    "       'price_change_percentage_60d', 'price_change_percentage_200d',\n",
    "       'price_change_percentage_1y'])\n",
    "\n",
    "# Copy the crypto names from the original DataFrame\n",
    "df_market_data_scaled['coin_id'] = df_market_data.index\n",
    "\n",
    "# Set the coin_id column as index\n",
    "df_market_data_scaled = df_market_data_scaled.set_index('coin_id')\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "df_market_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market_data_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardizing data the mean values are closer to zero and standard deviation for each feature is approximately 1.012423 which confirms that scaled data is centered around 0 with unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Value for k Using the Original Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the number of k-values from 1 to 11\n",
    "k= list(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create an empty list to store the inertia values\n",
    "inertia=[]\n",
    "\n",
    "# Create a for loop to compute the inertia with each possible value of k\n",
    "# Inside the loop:\n",
    "# 1. Create a KMeans model using the loop counter for the n_clusters\n",
    "# 2. Fit the model to the data using `df_market_data_scaled`\n",
    "# 3. Append the model.inertia_ to the inertia list\n",
    "for i in k:\n",
    "    k_model = KMeans(n_clusters=i,random_state=1)\n",
    "    k_model.fit(df_market_data_scaled)\n",
    "    inertia.append(k_model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the data to plot the Elbow curve\n",
    "elbow_data = {'k':k,'inertia':inertia}\n",
    "\n",
    "# Create a DataFrame with the data to plot the Elbow curve\n",
    "elbow_df = pd.DataFrame(elbow_data)\n",
    "elbow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a line chart with all the inertia values computed with\n",
    "# the different values of k to visually identify the optimal value for k.\n",
    "elbow_original_scaled = elbow_df.hvplot.line(x='k',\n",
    "                     y='inertia',\n",
    "                     title='Elbow Curve - Original Scaled Data',\n",
    "                     xticks='k')\n",
    "elbow_original_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "**Question:** What is the best value for `k`?\n",
    "\n",
    "**Answer:** The resulting elbow graph shows two sharp points, out of which 4 seems to be the optimal K value with lower inertia of 79.022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Cryptocurrencies with K-means Using the Original Scaled DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the K-Means model using the best value for k\n",
    "k_model = KMeans(n_clusters=4,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-Means model using the scaled DataFrame\n",
    "k_model.fit(df_market_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the clusters to group the cryptocurrencies using the scaled DataFrame\n",
    "k_model_clusters = k_model.predict(df_market_data_scaled)\n",
    "\n",
    "# Print the resulting array of cluster values.\n",
    "print(k_model_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the scaled DataFrame\n",
    "k_model_prediction = df_market_data_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the copy of the scaled DataFrame with the predicted clusters\n",
    "k_model_prediction['k_clusters_4'] = k_model_clusters\n",
    "\n",
    "# Display the copy of the scaled DataFrame\n",
    "k_model_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot using hvPlot by setting\n",
    "# `x=\"price_change_percentage_24h\"` and `y=\"price_change_percentage_7d\"`.\n",
    "# Colour the graph points with the labels found using K-Means and\n",
    "# add the crypto name in the `hover_cols` parameter to identify\n",
    "# the cryptocurrency represented by each data point.\n",
    "\n",
    "sctter_original_scaled = k_model_prediction.hvplot.scatter(\n",
    "x='price_change_percentage_24h',\n",
    "y='price_change_percentage_7d',\n",
    "by='k_clusters_4',\n",
    "title=\"Cryptocurrency Clusters- Original Scaled Data :K=4 \",\n",
    "hover_cols='coin_id'\n",
    ")\n",
    "sctter_original_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot presents a clearer distribution of cryptocurrencies based on their price change behaviors, showing four distinct clusters.\n",
    "\n",
    "The Yellow cluster represents cryptocurrencies that are consistently increasing over both timeframes, short term (24 hrs) and long term (7 days) indicating steady growth.\n",
    "\n",
    "The Blue cluster contains cryptocurrencies that are either stable or experiencing slight decline, especially over 24 hours. \n",
    "\n",
    "The Green cluster shows highly volatile behavior with a dramatic decline in 24 hours but recovery over 7 days, indicating sudden market shocks.\n",
    "\n",
    "The Red cluster consists of a very small number of points, with modest +ve change over 24 hours and slightly +ve over 7 day period.\n",
    "\n",
    "This scatter plot reflects a slight positive correlation between 24 hours and 7 days price change, specially for orange cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise Clusters with Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA model instance and set `n_components=3`.\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PCA model with `fit_transform` to reduce the original scaled Dataframe\n",
    "# down to three principal components.\n",
    "market_data_pca = pca.fit_transform(df_market_data_scaled)\n",
    "\n",
    "# View the scaled PCA data\n",
    "market_data_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the explained variance to determine how much information\n",
    "# can be attributed to each principal component.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "**Question:** What is the total explained variance of the three principal components?\n",
    "\n",
    "**Answer:** The first, second and third principal components explain 37.2% , 34.7 and 17.6% of the variance respectively. The relatively even distribution across the first two components indicates that both are almost equally important in explaining the variability in the data. \n",
    "\n",
    "This PCA model with 3 principal components has captured 89.50% of variability in the original features, which means these three components effectively capture the most significant patterns in the dataset, providing a well-balanced approach to dimensionality reduction and information retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the PCA data.\n",
    "df_market_pca = pd.DataFrame(market_data_pca,columns=[\"PCA1\",'PCA2','PCA3'])\n",
    "\n",
    "# Copy the crypto names from the original scaled DataFrame\n",
    "df_market_pca['coin_id'] = df_market_data.index\n",
    "\n",
    "# Set the coin_id column as index\n",
    "df_market_pca = df_market_pca.set_index('coin_id')\n",
    "\n",
    "# Display the scaled PCA DataFrame\n",
    "df_market_pca.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Value for k Using the Scaled PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the number of k-values from 1 to 11\n",
    "k_pca=list(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the inertia values\n",
    "inertia =[]\n",
    "\n",
    "# Create a for loop to compute the inertia with each possible value of k\n",
    "# Inside the loop:\n",
    "# 1. Create a KMeans model using the loop counter for the n_clusters\n",
    "# 2. Fit the model to the data using `df_market_data_pca`\n",
    "# 3. Append the model.inertia_ to the inertia list\n",
    "for i in k_pca:\n",
    "    k_model = KMeans(n_clusters=i,random_state=0)\n",
    "    k_model.fit(df_market_pca)\n",
    "    inertia.append(k_model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the data to plot the Elbow curve\n",
    "elbow_data = {'k':k_pca, 'inertia':inertia}\n",
    "\n",
    "# Create a DataFrame with the data to plot the Elbow curve\n",
    "df_elbow_pca = pd.DataFrame(elbow_data)\n",
    "df_elbow_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line chart with all the inertia values computed with\n",
    "# the different values of k to visually identify the optimal value for k.\n",
    "elbow_pca_scaled = df_elbow_pca.hvplot.line(x='k',\n",
    "                    y='inertia',\n",
    "                    title='Elbow Curve- PCA Data')\n",
    "elbow_pca_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following questions: \n",
    "\n",
    "* **Question:** What is the best value for `k` when using the PCA data?\n",
    "\n",
    "  * **Answer:** The sharp elbow appears at K=4 where the drop in inertia starts to slightly flatten out. So the optimal k value is 4 using PCA data.\n",
    "\n",
    "\n",
    "* **Question:** Does it differ from the best k value found using the original data?\n",
    "\n",
    "  * **Answer:** Optimal K value ,determined from both the data sources ,original data and PCA scaled data,consistently reflects K=4. However, k=2 also looks like a candidate with sharp elbow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Cryptocurrencies with K-means Using the Scaled PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the K-Means model using the best value for k\n",
    "pca_model = KMeans(n_clusters=4,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-Means model using the scaled PCA DataFrame\n",
    "pca_model.fit(df_market_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the clusters to group the cryptocurrencies using the scaled PCA DataFrame\n",
    "pca_clusters = pca_model.predict(df_market_pca)\n",
    "\n",
    "# Print the resulting array of cluster values.\n",
    "print(pca_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the scaled PCA DataFrame\n",
    "market_pca_prediction = df_market_pca.copy()\n",
    "\n",
    "# Add a new column to the copy of the PCA DataFrame with the predicted clusters\n",
    "market_pca_prediction['pca_clusters_4'] = pca_clusters\n",
    "\n",
    "# Display the copy of the scaled PCA DataFrame\n",
    "market_pca_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot using hvPlot by setting\n",
    "# `x=\"PC1\"` and `y=\"PC2\"`.\n",
    "# Colour the graph points with the labels found using K-Means and\n",
    "# add the crypto name in the `hover_cols` parameter to identify\n",
    "# the cryptocurrency represented by each data point.\n",
    "\n",
    "scatter_pca_scaled = market_pca_prediction.hvplot.scatter(x='PCA1',\n",
    "                            y='PCA2',\n",
    "                            by='pca_clusters_4',\n",
    "                            hover_cols='coin_id',\n",
    "                                    title=\"CryptoCurrency Segment- Scaled PCA data: PCA=3,K=4\")\n",
    "scatter_pca_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise and Compare the Results\n",
    "\n",
    "In this section, you will visually analyse the cluster analysis results by contrasting the outcome with and without using the optimisation techniques.\n",
    "\n",
    "The PCA-based clustering effectively grouped cryptocurrencies into four distinct clusters, each exhibiting clear behavioral patterns such as stability, growth, and volatility. While k-means clustering without optimization also identified meaningful clusters, it was less effective at capturing distinct differences, particularly in high-dimensional data. Notably, the green and red clusters contained only a single data point, prompting further analysis using alternative clustering models and performance comparisons to achieve a more accurate and optimal k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Cryptocurrency from original data for each cluster based on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the DataFrame with assigned clusters\n",
    "PCA_cryptocurrencies_df = market_pca_prediction.reset_index()\n",
    "\n",
    "#Retrieve cryptocurrencies coin_id by cluster\n",
    "for cluster_label in PCA_cryptocurrencies_df['pca_clusters_4'].unique():\n",
    "    coin_ids_in_cluster = PCA_cryptocurrencies_df[PCA_cryptocurrencies_df['pca_clusters_4'] == cluster_label]['coin_id']\n",
    "    print(f\"Cluster {cluster_label} Cryptocurrencies: {coin_ids_in_cluster.values}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "PCA_cryptocurrencies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterring the data using AgglomerativeClustering and Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch_model = Birch(n_clusters=4)\n",
    "birch_predictions = birch_model.fit_predict(df_market_data_scaled)\n",
    "birch_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo_model =  AgglomerativeClustering(n_clusters=4)\n",
    "agglo_predictions = agglo_model.fit_predict(df_market_data_scaled)\n",
    "agglo_predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the cluster results from using Kmeans, AgglomerativeClustering, Birch and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creting new DataFrame to store clusters from individual Clustering models\n",
    "market_prediction_all = df_market_data_scaled.copy()\n",
    "\n",
    "market_prediction_all['KMeans_Segments'] = k_model_clusters\n",
    "market_prediction_all['Agglomerative_Segments'] = agglo_predictions\n",
    "market_prediction_all['Birch_Segments'] = birch_predictions\n",
    "market_prediction_all['PCA_Segments']= pca_clusters\n",
    "\n",
    "market_prediction_all[['KMeans_Segments','Agglomerative_Segments','Birch_Segments','PCA_Segments']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis of Segments:\n",
    "market_prediction_all[['KMeans_Segments','Agglomerative_Segments','Birch_Segments','PCA_Segments']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Analysis of Segments**\n",
    "\n",
    "KMeans and PCA show perfect correlation. This suggests that the clusters derived from KMeans are heavily aligned with the principal components, which likely means that the PCA-reduced data retains most of the clustering structure.\n",
    "\n",
    "Similarly, Agglomerative and Birch are perfectly correlated, showing that both hierarchical methods identify identical clusters.\n",
    "\n",
    "Given the perfect correlation between KMeans and PCA, as well as Agglomerative and Birch, it would be redundant to use both KMeans and PCA or Agglomerative and Birch in the final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the models based on the Calinski-Harabasz Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Create a list to store the scores\n",
    "score_kmeans = []\n",
    "score_agglomerative = []\n",
    "score_birch = []\n",
    "\n",
    "# Create a list of k values to test (number of clusters)\n",
    "k = list(range(2, 11))\n",
    "\n",
    "for i in k:\n",
    "    # KMeans clustering\n",
    "    kmeans_model = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans_labels = kmeans_model.fit_predict(df_market_data_scaled)\n",
    "    kmeans_score = metrics.calinski_harabasz_score(df_market_data_scaled, kmeans_labels)\n",
    "    score_kmeans.append(kmeans_score)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    agglo_model = AgglomerativeClustering(n_clusters=i)\n",
    "    agglo_labels = agglo_model.fit_predict(df_market_data_scaled)\n",
    "    agglo_score = metrics.calinski_harabasz_score(df_market_data_scaled, agglo_labels)\n",
    "    score_agglomerative.append(agglo_score)\n",
    "    \n",
    "    # Birch Clustering\n",
    "    birch_model = Birch(n_clusters=i)\n",
    "    birch_labels = birch_model.fit_predict(df_market_data_scaled)\n",
    "    birch_score = metrics.calinski_harabasz_score(df_market_data_scaled, birch_labels)\n",
    "    score_birch.append(birch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scores into a DataFrame for easy plotting with hvPlot\n",
    "df_scores = pd.DataFrame({\n",
    "    'K_Calinski': k,\n",
    "    'KMeans_Calinski': score_kmeans,\n",
    "    'Agglomerative_Calinski': score_agglomerative,\n",
    "    'Birch_Calinski': score_birch\n",
    "})\n",
    "df_scores.set_index('K_Calinski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfomance Analysis based on Calinski-Harabasz Score**\n",
    "\n",
    "Agglomerative Clustering consistently has the highest scores across most K values, especially for K=5 and higher. This suggests that Agglomerative Clustering forms the most distinct and well-separated clusters overall.\n",
    "\n",
    "**KMeans performs well when K=4, where it achieves the highest score of 32.46. Which is the highest among all the scors. However, for K=5 and above, its performance is generally lower than Agglomerative Clustering.**\n",
    "\n",
    "Birch shows competitive performance for higher k values , K=6, k=7, but its performance declines slightly for lower K values and as K increases, making it less effective for larger numbers of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the models based on the Silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the Silhouette scores\n",
    "silhouette_kmeans = []\n",
    "silhouette_agglomerative = []\n",
    "silhouette_birch = []\n",
    "\n",
    "# Create a list of k values to test (number of clusters)\n",
    "k_values = list(range(2, 11))\n",
    "\n",
    "for k in k_values:\n",
    "    # KMeans clustering\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_labels = kmeans_model.fit_predict(df_market_data_scaled)\n",
    "    kmeans_silhouette = metrics.silhouette_score(df_market_data_scaled, kmeans_labels)\n",
    "    silhouette_kmeans.append(kmeans_silhouette)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    agglo_model = AgglomerativeClustering(n_clusters=k)\n",
    "    agglo_labels = agglo_model.fit_predict(df_market_data_scaled)\n",
    "    agglo_silhouette = metrics.silhouette_score(df_market_data_scaled, agglo_labels)\n",
    "    silhouette_agglomerative.append(agglo_silhouette)\n",
    "    \n",
    "    # Birch Clustering\n",
    "    birch_model = Birch(n_clusters=k)\n",
    "    birch_labels = birch_model.fit_predict(df_market_data_scaled)\n",
    "    birch_silhouette = metrics.silhouette_score(df_market_data_scaled, birch_labels)\n",
    "    silhouette_birch.append(birch_silhouette)\n",
    "\n",
    "# Combine Silhouette scores into a DataFrame for easy plotting with hvPlot\n",
    "df_silhouette_scores = pd.DataFrame({\n",
    "    'K_sil': k_values,\n",
    "    'KMeans_sil': silhouette_kmeans,\n",
    "    'Agglomerative_sil': silhouette_agglomerative,\n",
    "    'Birch_sil': silhouette_birch\n",
    "})\n",
    "\n",
    "df_silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinigh the DataFrames to compare  Calinski-Harabasz scores and Silhouette scores\n",
    "df_scores = pd.concat([df_scores,df_silhouette_scores],axis=1)\n",
    "df_scores[['Agglomerative_sil', 'KMeans_sil','Birch_sil']] = df_scores[['Agglomerative_sil', 'KMeans_sil','Birch_sil']].round(2)*100\n",
    "df_scores[['Agglomerative_sil', 'KMeans_sil','Birch_sil']] = df_scores[['Agglomerative_sil', 'KMeans_sil','Birch_sil']].astype(int)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame into long format for hvPlot compatibility\n",
    "df_silhouette_melted = df_silhouette_scores.melt(id_vars=['K_sil'], var_name='Model', value_name='Silhouette_Score')\n",
    "\n",
    "# Plot using hvPlot\n",
    "df_silhouette_melted.hvplot.line(x='K_sil', y='Silhouette_Score', by='Model', width=800, height=400, \n",
    "                      title='Model Comparison Based on Silhouette Score', xlabel='Number of Clusters (k)', \n",
    "                      ylabel='Silhouette Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Silhouette Scores:**\n",
    "\n",
    "KMeans performs best for K=2, achieving the highest silhouette score of 72. However, its performance significantly drops as K increases, especially for K=3, where it has a score of 34.\n",
    "\n",
    "Agglomerative Clustering shows consistent silhouette scores across K=2 to K=4, but its performance dips for K greater than 5.\n",
    "\n",
    "Birch has similar performance to Agglomerative for small K values (up to K=4), but it performs poorly as the number of clusters increases (K=6 to K=10).\n",
    "\n",
    "**Conclusion Based on Silhouette Scores:\n",
    "KMeans outperforms the other models for K=2, but for larger K values, it does not maintain good cluster separation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Calinski-Harabasz Scores:**\n",
    "KMeans performs well when K=4, where it achieves the highest score of 32.46. However, for K=5 and above, its performance is generally lower than Agglomerative Clustering.\n",
    "\n",
    "**Observations for Silhouette Scores:**\n",
    "KMeans performs best for K=2, achieving the highest silhouette score of 72. However, its performance significantly drops as K increases, especially for K=3, where it has a score of 34.\n",
    "\n",
    "Based on both the scores Optimal value for K=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting clusters Cryptocurrencies with new K-means Using the PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Model using K=2\n",
    "k_model = KMeans(n_clusters=2,random_state=1)\n",
    "k_model.fit(df_market_data_scaled)\n",
    "k_model_clusters = k_model.predict(df_market_data_scaled)\n",
    "k_model_prediction = df_market_data_scaled.copy()\n",
    "k_model_prediction['k_clusters_4'] = k_model_clusters\n",
    "#k_model_prediction.head()\n",
    "\n",
    "sctter_original_scaled_c2 = k_model_prediction.hvplot.scatter(\n",
    "x='price_change_percentage_24h',\n",
    "y='price_change_percentage_7d',\n",
    "by='k_clusters_4',\n",
    "title=\"Cryptocurrency Clusters- Original Scaled Data :K=4 \",\n",
    "hover_cols='coin_id'\n",
    ")\n",
    "sctter_original_scaled_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the featurs based on PCA\n",
    "pca_model = KMeans(n_clusters=2,random_state=1)\n",
    "pca_model.fit(df_market_pca)\n",
    "pca_clusters = pca_model.predict(df_market_pca)\n",
    "print(pca_clusters)\n",
    "\n",
    "# Create a copy of the scaled PCA DataFrame\n",
    "market_pca_clusters = df_market_pca.copy()\n",
    "\n",
    "# Add a new column to the copy of the PCA DataFrame with the predicted clusters\n",
    "market_pca_prediction['pca_clusters_2'] = pca_clusters\n",
    "\n",
    "# Display the copy of the scaled PCA DataFrame\n",
    "market_pca_prediction.head()\n",
    "\n",
    "scatter_pca_scaled_c2 = market_pca_prediction.hvplot.scatter(x='PCA1',\n",
    "                            y='PCA2',\n",
    "                            by='pca_clusters_2',\n",
    "                            hover_cols='coin_id',\n",
    "                                    title=\"CryptoCurrency Segment- Scaled PCA data: PCA=3,K=2\")\n",
    "scatter_pca_scaled_c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "  * **Question:** After visually analysing the cluster analysis results, what is the impact of using fewer features to cluster the data using K-Means?\n",
    "\n",
    "  * **Answer:** After evaluating the Silhouette and Calinski-Harabasz scores, the results indicate that KMeans performs optimally with K=2, achieving a highest Silhouette score of 72. Applying K=2 and visualizing the cluster analysis revealed that reducing the number of features had a significant impact on the clustering outcome. Initially, the elbow curve suggested K=4 based on the original data. However, with the highest Silhouette score at K=2 and the application of PCA, the resulting plot demonstrated clearer and more precise clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite plot to contrast the Elbow curves\n",
    "composite_plot_elbow = elbow_original_scaled + elbow_pca_scaled\n",
    "composite_plot_elbow.cols(2) # Arrange in two columns (side by side)\n",
    "composite_plot_elbow.opts(title='Elbow Curve Comparison')\n",
    "composite_plot_elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite plot to contrast the clusters K=4\n",
    "composite_plot_scatter=sctter_original_scaled + scatter_pca_scaled\n",
    "composite_plot_scatter.cols(2) # Arrange in two columns (side by side)\n",
    "composite_plot_scatter.opts(title='Cryptocurrency Clusters Comparision  K=4')\n",
    "composite_plot_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite plot to contrast the clusters K=2\n",
    "composite_plot_scatter_k2=sctter_original_scaled_c2 + scatter_pca_scaled_c2\n",
    "composite_plot_scatter_k2.cols(2) # Arrange in two columns (side by side)\n",
    "composite_plot_scatter_k2.opts(title='Cryptocurrency Clusters Comparision  K=2')\n",
    "composite_plot_scatter_k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Relationship between 4 Clusters and indivisual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmean_cluster_group = k_model_prediction.groupby('k_clusters_4').mean()\n",
    "\n",
    "# Step 3: Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(kmean_cluster_group, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Heatmap of KMean Clusters vs. Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap visualizes the relationship between KMeans clusters and various features related to cryptocurrency price changes over different time periods (e.g., 24h, 7d, 30d). Here are some key insights:\n",
    "\n",
    "Cluster 0 appears to have relatively stable values across all time periods, as indicated by the cool colors (blues), with little variation in price changes. This suggests that the assets in Cluster 0 show more stability over time.\n",
    "\n",
    "Cluster 1, on the other hand, shows significant variation, particularly in the mid-term periods (14d, 30d, 60d, 200d), as indicated by the warmer colors (reds and oranges). This implies that the assets in Cluster 1 are experiencing greater volatility, particularly in these time frames.\n",
    "\n",
    "The most significant difference between the two clusters is observed in the 30-day price change and 60-day price change, where Cluster 1 exhibits high levels of volatility.\n",
    "\n",
    "In summary, Cluster 0 represents more stable assets, while Cluster 1 captures assets with higher volatility, particularly over mid-term periods. Further analysis could explore the specific behaviors driving this volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the DataFrame with assigned clusters\n",
    "market_pca_clusters_k2 = market_pca_prediction.reset_index()\n",
    "\n",
    "#Inspect the DataFrame with assigned clusters\n",
    "PCA_cryptocurrencies_df = market_pca_clusters_k2.reset_index()\n",
    "\n",
    "#Retrieve cryptocurrencies coin_id by cluster\n",
    "for cluster_label in market_pca_clusters_k2['pca_clusters_2'].unique():\n",
    "    coin_ids_in_cluster = market_pca_clusters_k2[market_pca_clusters_k2['pca_clusters_2'] == cluster_label]['coin_id']\n",
    "    print(f\"Cluster {cluster_label} Cryptocurrencies: {coin_ids_in_cluster.values}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "market_pca_clusters_k2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretation of the PCA Component Loadings Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fit PCA\n",
    "pca = PCA(n_components=3)  # Choose the number of components you want to retain\n",
    "pca.fit(market_data_scaled)\n",
    "\n",
    "# Step 3: Create a DataFrame to show which features contribute to each PCA component\n",
    "# Access PCA components (weights of each feature for each principal component)\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_, \n",
    "    columns=df_market_data.columns,  # Original feature names as column headers\n",
    "    index=[f'PCA{i+1}' for i in range(pca.n_components_)]  # Name each component (PCA1, PCA2, ...)\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"PCA Component Loadings:\")\n",
    "print(pca_components)\n",
    "\n",
    "# If you want to visualize it:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pca_components, annot=True, cmap='coolwarm')\n",
    "plt.title('PCA Component Loadings')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Principal Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Relationships: The heatmap provides insights into how original features relate to the underlying principal components derived through PCA.\n",
    "\n",
    "PCA Component Utility: PCA1 and PCA2 capture essential market dynamics. PCA1 is crucial for short-term trends, while PCA2 is more reflective of mid-term performance.\n",
    "\n",
    "Strategic Decisions: Investors can use this analysis to focus on short-term indicators (like 24h and 7d changes) when making quick trading decisions, while mid-term indicators (30d and 60d changes) might be more relevant for strategic positioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Analysis\n",
    "\n",
    "The Mid-Term Cluster - yellow cluster - Cluster 2 (k=4) highlights promising investment opportunities in cryptocurrencies that demonstrate consistent growth across both short-term (24 hours) and long-term (7 days) timeframes. This cluster indicates steady upward momentum, making these assets particularly attractive for investors seeking reliable performance.\n",
    "\n",
    "This includes notable cryptocurrencies such as **Bitcoin, Ethereum, Bitcoin Cash, Binance Coin, Chainlink, Cardano, Litecoin, Monero, Tezos, Cosmos, Wrapped Bitcoin, Zcash, and Maker.**  These assets show potential for sustained growth over a mid-term horizon, further supporting strategic investment decisions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
